{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bse_mnist_digits_fcnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrjcs/iitpbse/blob/master/dl/d2/bse_mnist_digits_fcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2IrTN4gCcew"
      },
      "source": [
        "# A Basic Fully Connected Neural Network for MNIST Digit Classification\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70EjjPgoKkB9"
      },
      "source": [
        "## -- Keras provides in-built support to many datasets\n",
        "## -- nice documentation is available at https://keras.io/\n",
        "## -- such as MNIST (Modified National Institute of Standards and Technology database) @ http://yann.lecun.com/exdb/mnist/\n",
        "\t# database of handwritten digits\n",
        "\t# used  extensively in optical character recognition and machine learning research\n",
        "\t# training set of 60,000 examples, and a test set of 10,000 examples\n",
        "\t# digits have been size-normalized and centered in a fixed-size image\n",
        "\t# black and white digits\n",
        "\t# 28 x 28  pixels\n",
        "\t# Keras provides method to load MNIST data set\n",
        "  \n",
        "  > refer to # https://keras.io/datasets/#mnist-database-of-handwritten-digits for more details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKWVb27vAeL7"
      },
      "source": [
        "# load MNIST data set\n",
        "from keras.datasets import mnist\t \t#importing dataset\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data() \t#Keras function to load and split dataset into training and test data\n",
        "\n",
        "print (\"mnist data downloaded...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuUhUPFTAjbq"
      },
      "source": [
        "# this code cell is for visualization only\n",
        "\n",
        "import matplotlib.pyplot as plt\t\t\t#to plot images\n",
        "\t\n",
        "plt.imshow(X_train[50], cmap=plt.get_cmap('gray')) # ploting first image of training data set\n",
        "#plt.imshow(X_test[244], cmap=plt.get_cmap('gray'))\t# ploting 2445th image of test date set\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfwO2jCdAghp"
      },
      "source": [
        "# Print shape of dataset..it will print three tuples, namely the no. of images in dataset, height and width(60000, 28, 28)\n",
        "\n",
        "print (X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Uz0kTQAmtB"
      },
      "source": [
        "\n",
        "# Step 3: Preprocess input data for Keras\n",
        "\n",
        "X_temp = X_test\n",
        "\n",
        "# flatten 28*28 images to a 784 vector for each image and pixel precision set to 32 bit\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# Step 4: Preprocess class labels\n",
        "# check shape of our class label data\n",
        "\n",
        "print (Y_train.shape)\n",
        "#We should have 10 different classes, one for each digit, but it looks like we only have a 1-dimensional array.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU5LFemhAsiA"
      },
      "source": [
        "#check labels for the first 10 training samples:\n",
        "print (Y_train[:10])\n",
        "# output of the form [5 0 4 1 9 2 1 3 1 4]\n",
        "#The Y_train and Y_test data are not split into 10 distinct class labels, but rather are represented as a single array with the class values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MKhIS3vAuW9"
      },
      "source": [
        "\n",
        "from keras.utils import np_utils\t\t#for transforming data \n",
        "\n",
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "num_classes = Y_test.shape[1]\n",
        "\n",
        "# check again\t\n",
        "print (Y_train.shape)\n",
        "# (60000, 10)\n",
        "print (Y_train[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDjnsR5vLdqL"
      },
      "source": [
        "\n",
        "### A very simple model is being created in next few lines...this is a crucial step => creating a good network\n",
        "\n",
        "# Keras Models\n",
        "\n",
        "> Core data structure of Keras\n",
        "\n",
        "> A way to organize layers\n",
        "\n",
        "There are three ways to create Keras models:\n",
        "\n",
        "> Sequential model\n",
        "  >> A simple list of layers\n",
        "\n",
        "  >> Stacked\n",
        "\n",
        "  >> Single-input and single-output stacks of layers\n",
        "\n",
        "> The Functional API and Model subclassing\n",
        "\n",
        "> Refer to https://keras.io/api/models/ for more details\n",
        "\n",
        "### Use sequential model\n",
        "> Details @ https://keras.io/guides/sequential_model/\n",
        "\n",
        ">  A Sequential model is declared as\n",
        ">>        model = Sequential()\n",
        "then dense layers are added\n",
        "\n",
        "\n",
        "> Dense implements the operation: output = activation(dot(input, kernel) + bias) \n",
        "\n",
        "           >>  activation is the element-wise activation function passed as the activation argument\n",
        "\n",
        "           >>  kernel is a weights matrix created by the layer\n",
        "\n",
        "           >>  bias is a bias vector created by the layer (only applicable if use_bias is True)\n",
        "      \n",
        "> Adding layers (can be combined with layer declaration as well)\n",
        ">>         model = Sequential([Dense(32, input_shape=(784,)), Activation('relu'),Dense(10), Activation('softmax'),])\n",
        " \n",
        "\n",
        ">> > Or\n",
        "\n",
        ">>         model.add(Dense(32, input_dim=784))\n",
        ">>         model.add(Activation('relu'))\n",
        "\n",
        "> Generally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights\n",
        "\n",
        ">> First layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape\n",
        "\n",
        "\n",
        "> Dense(32, input_dim=784) specifies that it is \n",
        "\t\t>> first (input) layer\n",
        "        \n",
        "  >> output dimension is 32 ($1^{st}$ argument \n",
        "    \n",
        "  >> input dimension is 784\n",
        "\n",
        "> kernel_initializer: Initializations define the way to set the initial random weights of Keras layers.\n",
        "    \n",
        "   >> kernel_initializer='normal': name of initialization function for the weights of the layer. normal for values \n",
        "    \n",
        "   >> randomly drawn from normal distribution.\n",
        "   \n",
        "   >> many more intializers: Zeros, Ones, normal, Constant, normal , and many more\n",
        "    \n",
        "> If no activation function specified, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
        "\n",
        "\n",
        "  >> activations: Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers\n",
        "      \n",
        "  >> many activation function are available in Keras: relu, softmax, sigmoid, tanh, so on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKcp5iXTAyyZ"
      },
      "source": [
        "# Define model architecture\n",
        "\n",
        "from keras.models import Sequential\t\t#model\n",
        "from keras.layers import Dense\t\t\t#layer\n",
        "from keras.layers import Dropout\t\t#layer\n",
        "from keras import initializers      # for importing initializers of keras\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer=initializers.RandomNormal(), activation='relu')) #only one hidden layer with relu as activation function\n",
        "#model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer=initializers.Constant(value=5), activation='relu')) #only one hidden layer with relu as activation function\t\n",
        "model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\t\t\t\t\t#output layer with softmax as activation function\n",
        "\n",
        "\n",
        "#print(model.summary())\n",
        "\n",
        "print (\"congrts model defined...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhFx3K1jlf1P"
      },
      "source": [
        "**Additional**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "=> Improving Performance of Simple Network: additional hidden layers (add one more dense layer)\n",
        " \n",
        "     model.add(Dense(num_classes, kernel_initializer='normal', activation='relu'))\n",
        "     \n",
        "=> Improving Performance of Simple Network: additional hidden layers (add one more dense layer)\n",
        "\n",
        "     model.add(Dense(num_classes, kernel_initializer='normal', activation='tanh'))\n",
        "\n",
        "=> Improving Performance of Simple Network: introducing dropout layer\n",
        "\n",
        "      model.add(Dropout(0.2))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yknKONfAdiJx"
      },
      "source": [
        "# Once a model is \"built\", summary() method can be used to display its contents:\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZOI0ZQSM-Nv"
      },
      "source": [
        "### Before training, configure the learning process, using compile() method. Three argements:\n",
        "    > loss function: the objective function that model try to minimize\n",
        "          >> many more: categorical_crossentropy, mean_squared_error, mean_squared_logarithmic_error, ......\n",
        "\n",
        "    > optimizer: ANN training process is an optimization task with the aim of finding a set of weights to minimize some \n",
        "      >> objective function\n",
        "      >> determine how weights are updated\n",
        "      >> many more: adam (Adaptive moment estimation), sgd (Stochastic gradient descent)\n",
        "\n",
        "    > list of metrics: used to judge performance of model, similar to objective function however not used for training purpose\n",
        "      \n",
        "### optimizer, loss function, meterics => very important step which will determine the performance of your network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFg3NWWpA04y"
      },
      "source": [
        "# Compile model: Configures the model for training.\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "print (\"Compilation done ...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4bnbsYNSM4"
      },
      "source": [
        "# Training, Validation, and Test Data\n",
        "\n",
        "> training dataset => parameter tuning (e.g. weight) or learning\n",
        "\n",
        "> validation set => hyperparameter tuning (e.g. architecture)\n",
        "\n",
        "> test set => evaluation\n",
        "\n",
        "> epoch: number of times learning algorithm sees entire data\n",
        "\n",
        "> batch size: number of samples processed before updating weights\n",
        "\n",
        "> By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch. (no information, animated bar, numbe of epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSh9LnquA22y"
      },
      "source": [
        "#Train model\n",
        "\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "\n",
        "history =model.fit(X_train, Y_train,validation_split=0.2, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "print (\"parameter tuning done...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnr8rI_bc4ca"
      },
      "source": [
        "# about training \n",
        "history.history.keys()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vbhb5emdEQg"
      },
      "source": [
        "# Accuracy with the epochs\n",
        "\n",
        "plt.plot(history.history['accuracy'],'r')\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training'], loc='center right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVycUDFZdPKT"
      },
      "source": [
        "# Loss with epochs\n",
        "\n",
        "plt.plot(history.history['loss'],'g')\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_u0OQ53A4_x"
      },
      "source": [
        "# Step 8: Evaluate model\n",
        "scores = model.evaluate(X_test, Y_test)\n",
        "print(\"Error: %.2f%%\" % (100-scores[1]*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa-y7lxnm0t5"
      },
      "source": [
        "#printing metrices\n",
        "print(model.metrics_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TTXar8dFT7Z"
      },
      "source": [
        "# print summary of the model\n",
        "print (model.summary())\n",
        "\n",
        "# for more on model visualization you may refer to https://keras.io/visualization/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QajiyIsMfPtm"
      },
      "source": [
        "#pop method: Removes the last layer in the model\n",
        "#model.pop()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQd7kk4Ei0sL"
      },
      "source": [
        "plt.imshow(X_temp[2], cmap=plt.get_cmap('gray'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G2Amjm4gLye"
      },
      "source": [
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions[2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUgZpLvDlxRi"
      },
      "source": [
        "=> Improving Performance of Simple Network: using different optimizers: SGD, Adagrad,Adam...\n",
        "\n",
        "=> Improving Performance of Simple Network: training for more number of epochs (20)\n",
        "\n",
        "=> other options to explore\n",
        "\n",
        "\n",
        "> different learning rate for optimizer\n",
        "\n",
        "> number of neurons in hidden layer\n",
        "\n",
        "> batch size\n",
        "\n",
        "> with different optimizers\n",
        "   \n",
        "> Increasing the number of internal hidden neurons\n",
        "   \n",
        "  \n",
        "=> steps to follow to make an efficient image classifier?\n",
        "     \n",
        "     >lot of experimentation and testing to get the optimal structure and parameters"
      ]
    }
  ]
}