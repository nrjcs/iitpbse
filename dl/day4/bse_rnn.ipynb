{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bse_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOOHIfzSOxAilWvSD2WEBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrjcs/iitpbse/blob/master/dl/day4/bse_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_04DH5JQbka"
      },
      "source": [
        "**Dataset**\n",
        "\n",
        "\n",
        "\n",
        "> Large Movie Review Dataset or IMDB Dataset\n",
        "\n",
        "> A dataset for binary sentiment classification\n",
        "\n",
        "> A set of 25,000 highly polar movie reviews for training, and 25,000 for testing\n",
        "\n",
        "> Labeled by sentiment (positive/negative)\n",
        "\n",
        "> Additional details @ https://ai.stanford.edu/%7Eamaas/data/sentiment/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf2peKntPgXJ"
      },
      "source": [
        "from keras.datasets import imdb #importing imdb dataset from keras...details @ https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data\n",
        "\n",
        "# Reviews have been preprocessed\n",
        "# Each review is encoded as a list of word indexes (integers) ==> A vector\n",
        "# For convenience, words are indexed by overall frequency in the dataset\n",
        "# For instance, the integer \"3\" encodes the 3rd most frequent word in the data\n",
        "# As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word\n",
        "\n",
        "\n",
        "# imdb.load_data( ) method to load the dataset\n",
        "  # num_words => integer or None. Words are ranked by how often they occur (in the training set) and only the num_words most frequent \n",
        "    # words are kept. If None, all words are kept. \n",
        "    # Defaults to None.\n",
        "  # skip_top =>\tskip the top N most frequently occurring words (which may not be informative). \n",
        "  \n",
        "num_words = 5000\n",
        "skip_top = 5\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words = num_words, skip_top = skip_top)\n",
        "\n",
        "print (\"....Done....\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRSdnFAySaoG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# about dataset\n",
        "print (X_train.shape)\n",
        "print (Y_train.shape)\n",
        "print (X_test.shape)\n",
        "print (Y_test.shape)\n",
        "\n",
        "print (X_train[0])\n",
        "print (np.shape(X_train[0]))\n",
        "print (Y_train[0:10])\n",
        "\n",
        "print (\"....Done....\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4JNB5OtsAOp"
      },
      "source": [
        "print (X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GaLQbnhmbFq"
      },
      "source": [
        "# input can be of any length...however, the length of vectors is supposed to be the same so as to work with these vectors in Keras\r\n",
        "\r\n",
        "max_len = 500 # defining maximum length for each review\r\n",
        "\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0) details @ https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\r\n",
        "# Pads sequences to the same length.\r\n",
        "# Transforms a list (of length num_samples) of sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps)\r\n",
        "# num_timesteps is either the maxlen argument if provided, or the length of the longest sequence in the list. \r\n",
        "# padding/truncating = pre/post\r\n",
        "\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\r\n",
        "\r\n",
        "\r\n",
        "print (X_train.shape)\r\n",
        "print (Y_train.shape)\r\n",
        "print (X_test.shape)\r\n",
        "\r\n",
        "print (X_train[0])\r\n",
        "print (np.shape(X_train[0]))\r\n",
        "print (Y_train[0:10])\r\n",
        "\r\n",
        "print (\"....Done....\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLcNBblNsOBq"
      },
      "source": [
        "print (X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcpY8DV5sVVa"
      },
      "source": [
        "# define model\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import SimpleRNN\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers.embeddings import Embedding\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEX-57FbvOTH"
      },
      "source": [
        "# Word embedding and Embedding layer in Keras\r\n",
        "# Bag of word model => Sparse representation of words in a text\r\n",
        "# Word embeddings => Dense representation of words \r\n",
        "  # words are represented by dense vectors\r\n",
        "  # words with the same meaning are likely to have similar representation\r\n",
        "\r\n",
        "\r\n",
        "# Keras provides an Embedding layer to turn positive integers (indexes) into dense vectors of fixed size.\r\n",
        "# e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\r\n",
        "# This layer can only be used as the first layer in a model details @ https://keras.io/api/layers/core_layers/embedding/\r\n",
        "\r\n",
        "# tf.keras.layers.Embedding(input_dim, output_dim, embeddings_initializer=\"uniform\", ..., input_length=None)\r\n",
        "# input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\r\n",
        "# output_dim: Integer. Dimension of the dense embedding\r\n",
        "# input_length: Length of input sequences\r\n",
        "\r\n",
        "embed_vc_len = 100\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(num_words, embed_vc_len, input_length=max_len))\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QRfHtpqzO_n"
      },
      "source": [
        "\r\n",
        "model.add(SimpleRNN(16)) # SimpleRNN layer details @ https://keras.io/api/layers/recurrent_layers/simple_rnn/\r\n",
        "\r\n",
        "\r\n",
        "# Classification problem with two classes => adding a Dense layer with a single neuron and a sigmoid activation function \r\n",
        "\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si7OFPL_22lW"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCKBIuk626_Y"
      },
      "source": [
        "# compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# train model\r\n",
        "batch_size = 200\r\n",
        "no_epochs = 5\r\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=no_epochs, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCoLNRLr4kY2"
      },
      "source": [
        "# about training \r\n",
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HZTde6o3CN0"
      },
      "source": [
        "# evaluate model\r\n",
        "scores = model.evaluate(X_test, Y_test)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAdpzSkk41IG"
      },
      "source": [
        "# Accuracy with the epochs\r\n",
        "import matplotlib.pyplot as plt\t\t\t#to plot images\r\n",
        "plt.plot(history.history['accuracy'],'r')\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['training'], loc='center right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvRUDBm0490m"
      },
      "source": [
        "# Loss with epochs\r\n",
        "\r\n",
        "plt.plot(history.history['loss'],'g')\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['training'], loc='upper right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR9V_2Xh73uF"
      },
      "source": [
        "############################\r\n",
        "# Alternative\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(num_words, embed_vc_len, input_length=max_len))\r\n",
        "\r\n",
        "# LSTM layer Keras .... details @ https://keras.io/api/layers/recurrent_layers/lstm/\r\n",
        "# LSTM(units, activation=\"tanh\", recurrent_activation=\"sigmoid\", ...)\r\n",
        "\r\n",
        "model.add(LSTM(16)) # An LSTM layer with n cells... dimensionality of the output space\r\n",
        "\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "# compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# train model\r\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=no_epochs, batch_size=batch_size)\r\n",
        "\r\n",
        "# evaluate model\r\n",
        "scores = model.evaluate(X_test, Y_test)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckMMjklsAkEz"
      },
      "source": [
        "############################\r\n",
        "# Alternative\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(num_words, embed_vc_len, input_length=max_len))\r\n",
        "\r\n",
        "model.add(SimpleRNN(128)) # SimpleRNN layer details @ https://keras.io/api/layers/recurrent_layers/simple_rnn/\r\n",
        "\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "# compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# train model\r\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=no_epochs, batch_size=batch_size)\r\n",
        "\r\n",
        "# evaluate model\r\n",
        "scores = model.evaluate(X_test, Y_test)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSPVxMiU8jrU"
      },
      "source": [
        "############################\r\n",
        "# Alternative\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(num_words, embed_vc_len, input_length=max_len))\r\n",
        "\r\n",
        "model.add(LSTM(128)) \r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "# compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# train model\r\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=no_epochs, batch_size=batch_size)\r\n",
        "\r\n",
        "# evaluate model\r\n",
        "scores = model.evaluate(X_test, Y_test)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur4Ewt-o6f9a"
      },
      "source": [
        "############################\r\n",
        "# Alternative\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(num_words, embed_vc_len, input_length=max_len))\r\n",
        "\r\n",
        "model.add(LSTM(16, return_sequences=True)) #return full sequence \r\n",
        "model.add(LSTM(16))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "# compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# train model\r\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=no_epochs, batch_size=batch_size)\r\n",
        "\r\n",
        "# evaluate model\r\n",
        "scores = model.evaluate(X_test, Y_test)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiNPVxtc8KKa"
      },
      "source": [
        "############################\r\n",
        "from keras.layers import Dropout\r\n",
        "# Alternative\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(num_words, embed_vc_len, input_length=max_len))\r\n",
        "\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(LSTM(64, return_sequences=True))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(LSTM(64))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "# compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "# train model\r\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=no_epochs, batch_size=batch_size)\r\n",
        "\r\n",
        "# evaluate model\r\n",
        "scores = model.evaluate(X_test, Y_test)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}